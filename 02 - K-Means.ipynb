{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c830de2c",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Neste notebook, exploraremos o algoritmo de clusterização K-Means, um dos mais fundamentais e amplamente utilizados em aprendizado não supervisionado. O objetivo do K-Means é particionar um conjunto de $N$ observações em $K$ clusters distintos, onde cada observação pertence ao cluster com a média (centróide) mais próxima.\n",
    "\n",
    "O algoritmo opera de forma iterativa para atribuir cada ponto de dado a um dos $K$ grupos com base nas características fornecidas. Os centróides são então recalculados como a média dos pontos de dados atribuídos a cada cluster. Este processo de duas etapas, atribuição e atualização, continua até que a alocação dos pontos aos clusters não mude mais, ou seja, até a convergência.\n",
    "\n",
    "### Conteúdos abordados:\n",
    "\n",
    "* **Fundamentação Matemática**: A função objetivo do K-Means.\n",
    "* **Implementação em Python**: Construção do algoritmo passo a passo usando apenas a biblioteca NumPy.\n",
    "* **Visualização de Dados**: Carregamento e preparação de um dataset real.\n",
    "* **Passos do Algoritmo**:\n",
    "    1.  Inicialização dos centróides.\n",
    "    2.  Atribuição de clusters.\n",
    "    3.  Atualização dos centróides.\n",
    "* **Visualização dos Resultados**: Análise gráfica dos clusters formados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4924bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula de importação de bibliotecas\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Estilo para os gráficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2c8e8",
   "metadata": {},
   "source": [
    "## 2. Fundamentação Matemática do K-Means\n",
    "\n",
    "O K-Means busca minimizar a variância intra-cluster, também conhecida como Inércia ou Soma dos Quadrados Dentro do Cluster (WCSS - Within-Cluster Sum of Squares). A função objetivo pode ser formalmente expressa como:\n",
    "\n",
    "$$ J = \\sum_{k=1}^{K} \\sum_{i \\in C_k} || \\mathbf{x}_i - \\boldsymbol{\\mu}_k ||^2 $$\n",
    "\n",
    "Onde:\n",
    "- $K$ é o número de clusters.\n",
    "- $C_k$ é o conjunto de todas as observações pertencentes ao cluster $k$.\n",
    "- $\\mathbf{x}_i$ é o vetor que representa a $i$-ésima observação.\n",
    "- $\\boldsymbol{\\mu}_k$ é o centróide (média) do cluster $k$.\n",
    "- $||\\mathbf{x}_i - \\boldsymbol{\\mu}_k||^2$ é a distância Euclidiana ao quadrado entre a observação $\\mathbf{x}_i$ e o centróide $\\boldsymbol{\\mu}_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c410c1a",
   "metadata": {},
   "source": [
    "### O Processo Iterativo de Otimização\n",
    "\n",
    "O algoritmo utiliza um procedimento iterativo de otimização conhecido como Expectation-Maximization (EM):\n",
    "1.  **Passo de Atribuição (Expectation)**: Cada observação é atribuída ao cluster cujo centróide é o mais próximo. Esta etapa define os limites de decisão.\n",
    "    $$ C_k = \\{ i : ||\\mathbf{x}_i - \\boldsymbol{\\mu}_k||^2 \\le ||\\mathbf{x}_i - \\boldsymbol{\\mu}_j||^2 \\quad \\forall j, 1 \\le j \\le K \\} $$\n",
    "2.  **Passo de Atualização (Maximization)**: Os centróedes são recalculados como a média de todas as observações atribuídas ao seu cluster. Esta etapa move os centróides para o centro de suas novas nuvens de pontos.\n",
    "    $$ \\boldsymbol{\\mu}_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_i $$\n",
    "\n",
    "Esses dois passos são repetidos até a convergência, que é alcançada quando as atribuições dos pontos de dados não mudam mais entre as iterações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a1558",
   "metadata": {},
   "source": [
    "## 3. Preparação dos Dados\n",
    "\n",
    "Para este exercício, utilizaremos o famoso dataset *Iris*, que contém medições de três espécies de flores. Embora este dataset seja classicamente usado para tarefas de classificação (supervisionadas), ele serve como um excelente exemplo para a clusterização, pois podemos avaliar visualmente a capacidade do K-Means em separar as espécies com base apenas em suas características.\n",
    "\n",
    "Vamos carregar os dados e selecionar duas características (comprimento e largura da sépala) para facilitar a visualização em um plano 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ee24b9",
   "metadata": {},
   "source": [
    "### 3.1 Análise Exploratória: Escolhendo as Melhores Features\n",
    "\n",
    "Como podemos saber quais features são mais promissoras para a clusterização? Uma técnica visual poderosa é o **gráfico de pares (pair plot)**. Este tipo de gráfico mostra um scatter plot para cada par de features do dataset. Ele nos permite inspecionar visualmente a separação entre os grupos em diferentes subespaços 2D dos dados.\n",
    "\n",
    "Para o K-Means, procuramos por pares de features que mostrem aglomerados de pontos visualmente distintos e compactos.\n",
    "\n",
    "Vamos usar a biblioteca `seaborn` para criar um `pairplot` do dataset Iris. Para tornar a visualização mais clara, vamos colorir os pontos de acordo com suas espécies reais. Em um problema real não supervisionado, não teríamos esses rótulos, mas aqui eles servem como um guia para validarmos nossa escolha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bc65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Carregar o dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y_true = iris.target\n",
    "\n",
    "# Para facilitar o uso com o Seaborn, vamos criar um DataFrame do Pandas\n",
    "iris_df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "iris_df['species'] = pd.Series(iris.target).map({0: '', 1: '', 2: ''}) #.map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Gerar o pairplot\n",
    "# O hue='species' colore os pontos de acordo com a espécie real da flor\n",
    "sns.pairplot(iris_df, hue='species', palette='viridis', diag_kind='kde')\n",
    "plt.suptitle('Pair Plot do Dataset Iris', y=1.02) # Ajusta o título para não sobrepor\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y_true = iris.target\n",
    "\n",
    "# Para fins de visualização, vamos usar apenas as duas primeiras features:\n",
    "# Comprimento da sépala (sepal length) e Largura da sépala (sepal width)\n",
    "X = X[:, 2:]\n",
    "\n",
    "# Visualizar os dados\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c='gray', marker='o', s=50, label='Dados Originais')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title('Dataset Iris - Características das Sépalas')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51ff83",
   "metadata": {},
   "source": [
    "## 4. Implementação do Algoritmo K-Means\n",
    "\n",
    "Agora, vamos construir uma classe `KMeans` do zero. A estrutura da classe conterá os métodos essenciais para a execução do algoritmo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd63d42",
   "metadata": {},
   "source": [
    "### 4.1 Definição da Classe\n",
    "\n",
    "A classe `KMeans` armazenará os hiperparâmetros, os centróides e os rótulos. Ela conterá os métodos para inicializar os centróides, atribuir clusters, atualizar os centróides e orquestrar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, n_clusters=3, max_iter=100, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.centroids = None\n",
    "        self.labels = None\n",
    "\n",
    "    def _initialize_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Inicializa os centróides selecionando K pontos aleatórios do dataset.\n",
    "        \"\"\"\n",
    "        np.random.seed(self.random_state)\n",
    "        n_samples = X.shape[0]\n",
    "        random_indices = np.random.choice(n_samples, self.n_clusters, replace=False)\n",
    "        self.centroids = X[random_indices]\n",
    "\n",
    "    def _assign_clusters(self, X):\n",
    "        \"\"\"\n",
    "        Atribui cada ponto de dado ao centróide mais próximo.\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        distances = np.zeros((n_samples, self.n_clusters))\n",
    "\n",
    "        for i, centroid in enumerate(self.centroids):\n",
    "            distances[:, i] = np.sum((X - centroid)**2, axis=1)\n",
    "        \n",
    "        self.labels = np.argmin(distances, axis=1)\n",
    "\n",
    "    def _update_centroids(self, X):\n",
    "        \"\"\"\n",
    "        Atualiza a posição de cada centróide com base na média dos pontos atribuídos a ele.\n",
    "        \"\"\"\n",
    "        new_centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        \n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_points = X[self.labels == i]\n",
    "            if len(cluster_points) > 0:\n",
    "                new_centroids[i] = np.mean(cluster_points, axis=0)\n",
    "            else:\n",
    "                new_centroids[i] = self.centroids[i]\n",
    "        \n",
    "        self.centroids = new_centroids\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Executa o algoritmo K-Means.\n",
    "        \"\"\"\n",
    "        self._initialize_centroids(X)\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            old_centroids = self.centroids.copy()\n",
    "            self._assign_clusters(X)\n",
    "            self._update_centroids(X)\n",
    "            if np.allclose(old_centroids, self.centroids):\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Atribui clusters para novos dados com base nos centróides aprendidos.\n",
    "        \"\"\"\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for i, centroid in enumerate(self.centroids):\n",
    "            distances[:, i] = np.sum((X - centroid)**2, axis=1)\n",
    "        \n",
    "        return np.argmin(distances, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699be72",
   "metadata": {},
   "source": [
    "## 5. Treinando o Modelo e Visualizando os Resultados\n",
    "\n",
    "Com a classe `KMeans` definida, o próximo passo é instanciá-la, treinar o modelo com nossos dados e, finalmente, visualizar os clusters que o algoritmo identificou."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66cc4a3",
   "metadata": {},
   "source": [
    "### 5.1 Instanciando e Treinando o Modelo\n",
    "Primeiro, criamos uma instância da nossa classe `KMeans`, especificando o número de clusters desejado ($K=3$, pois sabemos que o dataset Iris possui 3 espécies), e em seguida chamamos o método `fit` para executar o algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d20282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciar e treinar o modelo K-Means\n",
    "kmeans = KMeans(n_clusters=3, max_iter=150, random_state=42)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc9365",
   "metadata": {},
   "source": [
    "### 5.2 Extraindo os Resultados\n",
    "Após o treinamento, os centróides finais e os rótulos de cada ponto de dado estão armazenados dentro do objeto `kmeans`. Vamos extraí-los para poder visualizá-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95daf311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter os centróides finais e os rótulos dos clusters\n",
    "final_centroids = kmeans.centroids\n",
    "predicted_labels = kmeans.labels\n",
    "\n",
    "print(\"Coordenadas dos Centróides Finais:\")\n",
    "print(final_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef75f48f",
   "metadata": {},
   "source": [
    "### 5.3 Visualização dos Clusters\n",
    "Com os rótulos e centróides em mãos, podemos gerar um gráfico de dispersão para visualizar os clusters identificados pelo algoritmo. Cada cluster será representado por uma cor diferente, e os centróides finais serão destacados com um marcador especial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d6ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar a figura para plots lado a lado\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# Plot 1: Clusters encontrados pelo K-Means\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=predicted_labels, s=50, cmap='viridis', edgecolor='k')\n",
    "axes[0].scatter(final_centroids[:, 0], final_centroids[:, 1], s=250, marker='*', c='red', edgecolor='black', label='Centróides')\n",
    "axes[0].set_title('Clusters Encontrados pelo K-Means')\n",
    "axes[0].set_xlabel('Comprimento da Sépala (cm)')\n",
    "axes[0].set_ylabel('Largura da Sépala (cm)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot 2: Classes Reais do Dataset\n",
    "scatter = axes[1].scatter(X[:, 0], X[:, 1], c=y_true, s=50, cmap='viridis', edgecolor='k')\n",
    "axes[1].set_title('Classes Reais (Ground Truth)')\n",
    "axes[1].set_xlabel('Comprimento da Sépala (cm)')\n",
    "axes[1].set_ylabel('Largura da Sépala (cm)')\n",
    "axes[1].legend(handles=scatter.legend_elements()[0], labels=iris.target_names.tolist())\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.suptitle('Comparação: K-Means vs. Rótulos Reais', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee707fe",
   "metadata": {},
   "source": [
    "### 5.4 Contagem de Acertos\n",
    "\n",
    "Como os rótulos dos clusters (0, 1, 2...) são arbitrários e não correspondem diretamente aos rótulos reais das espécies, não podemos simplesmente compará-los.\n",
    "\n",
    "Uma forma intuitiva de medir o resultado é verificar a \"pureza\" de cada cluster. Para cada cluster, vamos assumir que ele representa a espécie de flor que aparece com mais frequência dentro dele. Em seguida, contamos quantos pontos de dados nesse cluster realmente pertencem a essa espécie majoritária. A soma desses valores nos dará o número total de \"acertos\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "\n",
    "correct_predictions = 0\n",
    "n_samples = X.shape[0]\n",
    "\n",
    "# Para cada label de cluster (0, 1, 2...)\n",
    "for i in range(kmeans.n_clusters):\n",
    "    # 1. Encontra todos os pontos de dados que foram atribuídos ao cluster 'i'\n",
    "    mask = (predicted_labels == i)\n",
    "    \n",
    "    # 2. Dentre esses pontos, descobre qual é a classe real mais comum (a moda)\n",
    "    # Se a maioria dos pontos no cluster 'i' for da classe real 2, assumimos que 'i' corresponde a 2\n",
    "    dominant_label = mode(y_true[mask], keepdims=True)[0][0]\n",
    "    \n",
    "    # 3. Conta quantos pontos nesse cluster realmente pertencem a essa classe dominante\n",
    "    hits = np.sum(y_true[mask] == dominant_label)\n",
    "    \n",
    "    # 4. Adiciona essa contagem ao total de acertos\n",
    "    correct_predictions += hits\n",
    "\n",
    "print(f\"Número de acertos: {correct_predictions} de {n_samples} pontos.\")\n",
    "print(f\"Taxa de acerto: {(correct_predictions / n_samples):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5cdf49",
   "metadata": {},
   "source": [
    "### 5.4 Visualização das Regiões de Decisão\n",
    "Os centróides definem fronteiras no espaço de características. Qualquer ponto que cair dentro de uma determinada região será atribuído ao centróide correspondente. Essas regiões formam um Diagrama de Voronoi.\n",
    "\n",
    "Para visualizar isso, podemos criar uma malha de pontos (meshgrid) que cobre todo o espaço do gráfico, usar nosso modelo treinado para prever o cluster de cada ponto na malha e, em seguida, colorir o fundo de acordo com essas previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d064c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma malha para cobrir o espaço do gráfico\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "\n",
    "# Usar o modelo para prever o cluster de cada ponto na malha\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plotar o resultado\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.contourf(xx, yy, Z, alpha=0.3, cmap='viridis')\n",
    "plt.scatter(X[:, 0], X[:, 1], c=predicted_labels, s=50, edgecolor='k')\n",
    "plt.scatter(final_centroids[:, 0], final_centroids[:, 1], s=250, marker='*', c='red', edgecolor='black', label='Centróides')\n",
    "plt.title('Regiões de Decisão do K-Means')\n",
    "plt.xlabel('Comprimento da Sépala (cm)')\n",
    "plt.ylabel('Largura da Sépala (cm)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf36fc5",
   "metadata": {},
   "source": [
    "## 6. O Método do Cotovelo (Elbow Method)\n",
    "\n",
    "Até agora, definimos o número de clusters ($K$) como 3, pois já tínhamos um conhecimento prévio do dataset Iris. Em um problema real de aprendizado não supervisionado, o número ótimo de clusters é desconhecido.\n",
    "\n",
    "O Método do Cotovelo é uma heurística utilizada para nos ajudar a encontrar um bom valor para $K$. A ideia é executar o algoritmo K-Means para um intervalo de valores de $K$ (por exemplo, de 1 a 10) e, para cada execução, calcular a inércia (WCSS).\n",
    "\n",
    "$$ \\text{Inércia} = \\sum_{k=1}^{K} \\sum_{i \\in C_k} || \\mathbf{x}_i - \\boldsymbol{\\mu}_k ||^2 $$\n",
    "\n",
    "Ao plotar a inércia em função de $K$, o gráfico resultante geralmente se parece com um braço. O \"cotovelo\" nesse gráfico, o ponto onde a taxa de diminuição da inércia se torna abruptamente menor, é um bom candidato para o número ótimo de clusters. Isso ocorre porque adicionar mais clusters além do cotovelo não melhora significativamente a variância intra-cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc047b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementando o Método do Cotovelo com visualização iterativa\n",
    "k_range = range(1, 11)\n",
    "inertias = []\n",
    "clustering_results = []\n",
    "\n",
    "for k in k_range:\n",
    "    model = KMeans(n_clusters=k, max_iter=150, random_state=42)\n",
    "    model.fit(X)\n",
    "    \n",
    "    # Calcular a inércia (WCSS)\n",
    "    current_inertia = 0\n",
    "    for i in range(k):\n",
    "        # Seleciona os pontos pertencentes ao cluster i\n",
    "        cluster_points = X[model.labels == i]\n",
    "        # Calcula a soma das distâncias quadradas ao centróide do cluster i\n",
    "        current_inertia += np.sum((cluster_points - model.centroids[i])**2)\n",
    "    \n",
    "    inertias.append(current_inertia)\n",
    "    clustering_results.append({'labels': model.labels, 'centroids': model.centroids})\n",
    "\n",
    "# Plotar o gráfico do cotovelo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertias, 'bo-')\n",
    "plt.xlabel('Número de Clusters (K)')\n",
    "plt.ylabel('Inércia (WCSS)')\n",
    "plt.title('Método do Cotovelo para Encontrar o K Ótimo')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446bbce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar os resultados da clusterização para cada K\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8), sharex=True, sharey=True)\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, k in enumerate(k_range):\n",
    "    result = clustering_results[i]\n",
    "    labels = result['labels']\n",
    "    centroids = result['centroids']\n",
    "    \n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=labels, s=40, cmap='viridis', edgecolor='k')\n",
    "    axes[i].scatter(centroids[:, 0], centroids[:, 1], marker='*', s=150, c='red', edgecolor='black')\n",
    "    axes[i].set_title(f'K = {k}')\n",
    "    axes[i].grid(False)\n",
    "\n",
    "plt.suptitle('Visualização da Clusterização para Diferentes Valores de K', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90543d89",
   "metadata": {},
   "source": [
    "### Análise dos Gráficos\n",
    "\n",
    "O primeiro gráfico, o do cotovelo, mostra claramente uma dobra acentuada em $K=3$. A partir deste ponto, a redução na inércia ao adicionar mais um cluster é muito menor, indicando que $K=3$ é uma escolha razoável.\n",
    "\n",
    "A segunda série de gráficos reforça essa conclusão visualmente.\n",
    "* Com $K=1$, todos os dados pertencem a um único grupo.\n",
    "* Com $K=2$, o algoritmo separa o grupo mais distinto (setosa), mas ainda agrupa os outros dois (versicolor e virginica).\n",
    "* Com $K=3$, a separação corresponde muito bem à estrutura real dos dados.\n",
    "* Para $K>3$, o algoritmo começa a subdividir clusters que já são relativamente coesos, o que geralmente não é desejável e configura um \"overfitting\" do modelo de clusterização aos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8422f1",
   "metadata": {},
   "source": [
    "## 7. Aplicação 1: Compressão de Imagem (Quantização Vetorial)\n",
    "\n",
    "Uma aplicação fascinante e não óbvia do K-Means é a compressão de imagens. A técnica, formalmente conhecida como **Quantização Vetorial**, utiliza a clusterização para reduzir o número de cores em uma imagem, diminuindo assim o espaço necessário para armazená-la.\n",
    "\n",
    "A ideia central é a seguinte:\n",
    "1.  Uma imagem digital colorida é uma matriz de pixels, onde cada pixel é um vetor, tipicamente com 3 dimensões (Vermelho, Verde, Azul - RGB). Uma imagem com milhões de pixels pode ter centenas de milhares de cores únicas.\n",
    "2.  Podemos tratar cada pixel (vetor de cor) como um ponto de dado em um espaço 3D.\n",
    "3.  O K-Means é então utilizado para agrupar essas cores em $K$ clusters. Os centróides desses clusters formarão a nossa nova \"paleta de cores\", contendo apenas $K$ cores representativas.\n",
    "4.  Para comprimir a imagem, cada pixel original é substituído pela cor do centróide do cluster ao qual ele foi atribuído.\n",
    "\n",
    "O resultado é uma imagem que utiliza apenas $K$ cores, mas que ainda se assemelha muito à original. A compressão ocorre porque, em vez de armazenar o valor RGB de cada pixel, precisamos apenas armazenar a paleta de $K$ cores e, para cada pixel, um índice (de 0 a $K-1$) apontando para a cor correspondente na paleta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b27832",
   "metadata": {},
   "source": [
    "### 7.1 Carregando e Preparando a Imagem a partir de uma URL\n",
    "Primeiro, vamos carregar uma imagem de exemplo diretamente de uma URL usando a biblioteca `imageio`. Em seguida, vamos remodelar a matriz da imagem (altura x largura x 3 canais de cor) para um formato 2D (número de pixels x 3 canais), que é o formato esperado pela nossa implementação do K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d25066f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# URL de uma imagem de alta qualidade para o exemplo\n",
    "# (Exemplo: Uma paisagem com cores variadas)\n",
    "image_url = \"https://cdn.britannica.com/87/196687-138-2D734164/facts-parrots.jpg?w=800&h=450&c=crop\"\n",
    "\n",
    "# Ler a imagem diretamente da URL\n",
    "source_image = imageio.imread(image_url)\n",
    "\n",
    "# Normalizar os valores dos pixels para o intervalo [0, 1]\n",
    "source_image = source_image / 255.0\n",
    "\n",
    "# Obter as dimensões da imagem\n",
    "height, width, channels = source_image.shape\n",
    "\n",
    "# Remodelar a imagem para ser uma lista de pixels (vetores RGB)\n",
    "pixel_list = source_image.reshape(-1, channels)\n",
    "\n",
    "print(f\"Dimensões originais da imagem: {source_image.shape}\")\n",
    "print(f\"Dimensões da lista de pixels: {pixel_list.shape}\")\n",
    "print(f\"Número de cores únicas na imagem original: {len(np.unique(pixel_list, axis=0))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a1089",
   "metadata": {},
   "source": [
    "### 7.2 Aplicando K-Means para Criar a Paleta de Cores\n",
    "Agora, vamos aplicar nosso algoritmo K-Means aos dados dos pixels. Para a tarefa de compressão, um número maior de clusters (cores) tende a preservar mais detalhes. Escolheremos $K=16$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o número de clusters (cores na nova paleta)\n",
    "K_compression = 16\n",
    "\n",
    "# Instanciar e treinar um modelo K-Means específico para a compressão\n",
    "compression_kmeans = KMeans(n_clusters=K_compression, max_iter=100, random_state=42)\n",
    "compression_kmeans.fit(pixel_list)\n",
    "\n",
    "# Os centróides encontrados formam a nossa nova paleta de cores\n",
    "color_palette_compression = compression_kmeans.centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf0c30",
   "metadata": {},
   "source": [
    "### 7.3 Reconstruindo a Imagem Comprimida\n",
    "Com a paleta de 16 cores definida e cada pixel original atribuído a um cluster, podemos reconstruir a imagem. Criamos uma nova matriz de pixels onde cada pixel é substituído pela cor do centróide do seu cluster e, por fim, remodelamos a matriz de volta para as dimensões originais da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter os rótulos de cluster para cada pixel\n",
    "pixel_labels_compression = compression_kmeans.labels\n",
    "\n",
    "# Criar a imagem comprimida substituindo cada pixel pelo seu centróide\n",
    "compressed_pixel_list = color_palette_compression[pixel_labels_compression]\n",
    "\n",
    "# Remodelar a lista de pixels de volta para o formato de imagem original\n",
    "compressed_image = compressed_pixel_list.reshape(height, width, channels)\n",
    "\n",
    "# Visualizar a imagem original e a imagem comprimida lado a lado\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(source_image)\n",
    "axes[0].set_title(f'Imagem Original ({len(np.unique(pixel_list, axis=0))} cores)')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(compressed_image)\n",
    "axes[1].set_title(f'Imagem Comprimida ({K_compression} cores)')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55661888",
   "metadata": {},
   "source": [
    "## 8. Aplicação 2: Segmentação de Imagem\n",
    "A segmentação de imagem é o processo de particionar uma imagem em múltiplos segmentos (conjuntos de pixels), com o objetivo de simplificar sua representação para facilitar a análise.\n",
    "\n",
    "O K-Means pode ser usado para uma segmentação simples baseada em cores. Como o algoritmo agrupa pixels de cores semelhantes, os clusters resultantes correspondem naturalmente a diferentes regiões da imagem com perfis de cor distintos.\n",
    "\n",
    "Para esta tarefa, um número menor de clusters é geralmente mais útil, pois nosso objetivo é identificar as macro-regiões da imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9257b7",
   "metadata": {},
   "source": [
    "### 8.1 Treinando um Novo Modelo para Segmentação\n",
    "Vamos treinar um novo modelo K-Means, desta vez com um número menor de clusters ($K=6$), para identificar as principais regiões de cor na imagem. Usaremos os mesmos dados de pixels da imagem original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8decad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir um número menor de clusters para a segmentação\n",
    "K_segmentation = 6\n",
    "\n",
    "# Instanciar e treinar um novo modelo K-Means específico para a segmentação\n",
    "segmentation_kmeans = KMeans(n_clusters=K_segmentation, max_iter=100, random_state=42)\n",
    "segmentation_kmeans.fit(pixel_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f4c07",
   "metadata": {},
   "source": [
    "### 8.2 Visualizando o Mapa de Segmentação\n",
    "O array de `labels` resultante do nosso novo modelo contém o índice do segmento para cada pixel. Ao remodelar este array de volta às dimensões da imagem original, criamos um \"mapa de segmentação\", onde cada valor de pixel corresponde ao seu ID de segmento. Podemos colorir este mapa para uma visualização clara das regiões identificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter os rótulos e centróides do modelo de segmentação\n",
    "pixel_labels_segmentation = segmentation_kmeans.labels\n",
    "color_palette_segmentation = segmentation_kmeans.centroids\n",
    "\n",
    "# Criar a imagem segmentada (cada segmento com a cor do seu centróide)\n",
    "segmented_pixel_list = color_palette_segmentation[pixel_labels_segmentation]\n",
    "segmented_image = segmented_pixel_list.reshape(height, width, channels)\n",
    "\n",
    "# Visualizar a imagem original e a imagem segmentada\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 7))\n",
    "\n",
    "axes[0].imshow(source_image)\n",
    "axes[0].set_title('Imagem Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(segmented_image)\n",
    "axes[1].set_title(f'Imagem Segmentada em {K_segmentation} Regiões')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444c8c4",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "Neste exercício, você usará a classe `KMeans` que construímos para explorar um novo dataset e descobrir seus agrupamentos naturais.\n",
    "\n",
    "O dataset escolhido é o **\"Wine dataset\"**, que contém a análise química de vinhos cultivados na mesma região da Itália, mas derivados de três cultivares (tipos de uva) diferentes. O objetivo é ver se o K-Means consegue agrupar os vinhos de acordo com seu cultivar, usando apenas as suas características químicas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e18392",
   "metadata": {},
   "source": [
    "### Tarefa 1: Análise e Seleção de Features\n",
    "\n",
    "Primeiro, carregue o dataset `wine` e use a função `seaborn.pairplot` para visualizar as relações entre as features. Analise o gráfico e escolha o par de features que você acredita que melhor separa os 3 grupos. Plote um gráfico de dispersão apenas com o par selecionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12a5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_wine\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6456a409",
   "metadata": {},
   "source": [
    "### Tarefa 2: Encontrando o K Ótimo\n",
    "\n",
    "Aplique o Método do Cotovelo nas duas features que você escolheu. Calcule e plote a inércia (WCSS) para K de 1 a 10. Com base no seu gráfico, qual parece ser o número ideal de clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c885c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26588d60",
   "metadata": {},
   "source": [
    "### Tarefa 3: Clusterização e Avaliação\n",
    "\n",
    "Use o K encontrado na tarefa anterior para treinar seu modelo `KMeans`. Crie um gráfico com dois subplots: um mostrando os clusters encontrados pelo algoritmo e outro mostrando os dados com os rótulos reais para comparação. Por fim, calcule a taxa de acertos e comente o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50658d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
