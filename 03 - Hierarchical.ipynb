{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c830de2c",
   "metadata": {},
   "source": [
    "# Clusterização Hierárquica\n",
    "\n",
    "## 1. Introdução\n",
    "\n",
    "Neste notebook, exploraremos os algoritmos de clusterização hierárquica, uma família de métodos que constrói uma hierarquia de clusters organizando os dados em uma estrutura semelhante a uma árvore. Diferentemente do K-Means, que requer que especifiquemos o número de clusters antecipadamente, a clusterização hierárquica nos permite descobrir a estrutura natural dos dados em diferentes níveis de granularidade.\n",
    "\n",
    "A clusterização hierárquica pode ser dividida em duas abordagens principais:\n",
    "- **Aglomerativa (Bottom-up)**: Inicia com cada ponto como um cluster individual e, iterativamente, combina os clusters mais próximos até formar um único cluster.\n",
    "- **Divisiva (Top-down)**: Inicia com todos os pontos em um único cluster e, recursivamente, divide os clusters até que cada ponto forme seu próprio cluster.\n",
    "\n",
    "### Conteúdos abordados:\n",
    "\n",
    "* **Fundamentação Matemática**: Métricas de distância entre clusters e critérios de ligação.\n",
    "* **Implementação em Python**: Construção do algoritmo aglomerativo passo a passo usando NumPy.\n",
    "* **Dendrogramas**: Visualização da hierarquia de clusters.\n",
    "* **Critérios de Ligação**: Single, Complete, Average e Ward.\n",
    "* **Determinação do Número de Clusters**: Métodos para \"cortar\" o dendrograma.\n",
    "* **Aplicações Práticas**: Análise de dados reais e comparação com K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4924bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula de importação de bibliotecas\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris, load_wine\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Estilo para os gráficos\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a2c8e8",
   "metadata": {},
   "source": [
    "## 2. Fundamentação Matemática da Clusterização Hierárquica\n",
    "\n",
    "A clusterização hierárquica aglomerativa funciona com base em uma **matriz de distâncias** entre todos os pares de pontos (ou clusters). O algoritmo segue estes passos fundamentais:\n",
    "\n",
    "1. **Inicialização**: Cada observação $\\mathbf{x}_i$ forma um cluster individual $C_i = \\{\\mathbf{x}_i\\}$.\n",
    "\n",
    "2. **Cálculo da Matriz de Distâncias**: Para $N$ pontos, calculamos uma matriz simétrica $D \\in \\mathbb{R}^{N \\times N}$ onde $D_{ij}$ representa a distância entre os pontos $\\mathbf{x}_i$ e $\\mathbf{x}_j$:\n",
    "   $$ D_{ij} = d(\\mathbf{x}_i, \\mathbf{x}_j) $$\n",
    "\n",
    "3. **Iteração**: Em cada passo, encontramos o par de clusters $(C_i, C_j)$ com menor distância e os combinamos em um novo cluster $C_{ij} = C_i \\cup C_j$.\n",
    "\n",
    "4. **Atualização**: Recalculamos as distâncias do novo cluster para todos os outros clusters existentes.\n",
    "\n",
    "5. **Terminação**: O processo continua até que reste apenas um cluster contendo todas as observações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c410c1a",
   "metadata": {},
   "source": [
    "### Critérios de Ligação (Linkage)\n",
    "\n",
    "O ponto crucial da clusterização hierárquica é como definimos a distância entre dois clusters. Existem vários critérios de ligação:\n",
    "\n",
    "1. **Single Linkage (Ligação Simples)**:\n",
    "   $$ d(C_i, C_j) = \\min_{\\mathbf{x} \\in C_i, \\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
    "   A distância é definida pelos pontos mais próximos entre os clusters.\n",
    "\n",
    "2. **Complete Linkage (Ligação Completa)**:\n",
    "   $$ d(C_i, C_j) = \\max_{\\mathbf{x} \\in C_i, \\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
    "   A distância é definida pelos pontos mais distantes entre os clusters.\n",
    "\n",
    "3. **Average Linkage (Ligação Média)**:\n",
    "   $$ d(C_i, C_j) = \\frac{1}{|C_i||C_j|} \\sum_{\\mathbf{x} \\in C_i} \\sum_{\\mathbf{y} \\in C_j} d(\\mathbf{x}, \\mathbf{y}) $$\n",
    "   A distância é a média de todas as distâncias entre pares de pontos dos clusters.\n",
    "\n",
    "4. **Ward Linkage (Critério de Ward)**:\n",
    "   $$d(C_i, C_j) = \\frac{|C_i||C_j|}{|C_i|+|C_j|} \\|\\mathbf{m}_i - \\mathbf{m}_j\\|^2$$\n",
    "   Onde $\\mathbf{m}_i$ e $\\mathbf{m}_j$ são os centróides dos clusters $C_i$ e $C_j$, respectivamente, e $|C_k|$ é o número de pontos no cluster $C_k$.\n",
    "   Minimiza a variância intra-cluster ao combinar clusters. É baseado na soma dos quadrados das distâncias aos centróides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8a1558",
   "metadata": {},
   "source": [
    "## 3. Preparação dos Dados\n",
    "\n",
    "Vamos começar com um exemplo simples usando dados sintéticos para entender visualmente como funciona a clusterização hierárquica. Depois, aplicaremos o algoritmo ao dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando dados sintéticos simples para demonstração\n",
    "np.random.seed(42)\n",
    "X_simple = np.array([[1, 2], [1.5, 1.8], [5, 8], [8, 8], [1, 0.6], [9, 11]])\n",
    "\n",
    "# Visualizar os dados\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_simple[:, 0], X_simple[:, 1], c='blue', s=100, alpha=0.7)\n",
    "for i, (x, y) in enumerate(X_simple):\n",
    "    plt.annotate(f'P{i}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "plt.title('Dataset Simples para Demonstração')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Coordenadas dos pontos:\")\n",
    "for i, point in enumerate(X_simple):\n",
    "    print(f\"P{i}: {point}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51ff83",
   "metadata": {},
   "source": [
    "## 4. Implementação do Algoritmo Hierárquico Aglomerativo\n",
    "\n",
    "Vamos construir uma implementação simplificada do algoritmo hierárquico aglomerativo para entender seus passos fundamentais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalClustering:\n",
    "    def __init__(self, linkage='single'):\n",
    "        \"\"\"\n",
    "        Inicializa o algoritmo de clusterização hierárquica.\n",
    "        \n",
    "        Parameters:\n",
    "        linkage: str, critério de ligação ('single', 'complete', 'average')\n",
    "        \"\"\"\n",
    "        self.linkage = linkage\n",
    "        self.merge_history = []\n",
    "        self.distances = []\n",
    "        \n",
    "    def _calculate_distance_matrix(self, X):\n",
    "        \"\"\"\n",
    "        Calcula a matriz de distâncias entre todos os pares de pontos.\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        dist_matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                dist = np.linalg.norm(X[i] - X[j])\n",
    "                dist_matrix[i, j] = dist\n",
    "                dist_matrix[j, i] = dist\n",
    "                \n",
    "        return dist_matrix\n",
    "    \n",
    "    def _cluster_distance(self, cluster1, cluster2, X, dist_matrix):\n",
    "        \"\"\"\n",
    "        Calcula a distância entre dois clusters baseado no critério de ligação.\n",
    "        \"\"\"\n",
    "        if self.linkage == 'single':\n",
    "            # Distância mínima entre qualquer par de pontos dos clusters\n",
    "            min_dist = float('inf')\n",
    "            for i in cluster1:\n",
    "                for j in cluster2:\n",
    "                    if dist_matrix[i, j] < min_dist:\n",
    "                        min_dist = dist_matrix[i, j]\n",
    "            return min_dist\n",
    "            \n",
    "        elif self.linkage == 'complete':\n",
    "            # Distância máxima entre qualquer par de pontos dos clusters\n",
    "            max_dist = 0\n",
    "            for i in cluster1:\n",
    "                for j in cluster2:\n",
    "                    if dist_matrix[i, j] > max_dist:\n",
    "                        max_dist = dist_matrix[i, j]\n",
    "            return max_dist\n",
    "            \n",
    "        # elif self.linkage == 'average':\n",
    "        # Distância média entre todos os pares de pontos dos clusters\n",
    "        # ...\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Executa o algoritmo de clusterização hierárquica aglomerativa.\n",
    "        \"\"\"\n",
    "        n = len(X)\n",
    "        \n",
    "        # Inicializar cada ponto como um cluster\n",
    "        clusters = [[i] for i in range(n)]\n",
    "        \n",
    "        # Calcular matriz de distâncias inicial\n",
    "        dist_matrix = self._calculate_distance_matrix(X)\n",
    "        \n",
    "        self.merge_history = []\n",
    "        self.distances = []\n",
    "        \n",
    "        step = 0\n",
    "        print(f\"Passo inicial: {len(clusters)} clusters individuais\")\n",
    "        print(f\"Clusters: {clusters}\\n\")\n",
    "        \n",
    "        # Continuar até que reste apenas um cluster\n",
    "        while len(clusters) > 1:\n",
    "            # Encontrar o par de clusters mais próximo\n",
    "            min_distance = float('inf')\n",
    "            merge_i, merge_j = -1, -1\n",
    "            \n",
    "            for i in range(len(clusters)):\n",
    "                for j in range(i+1, len(clusters)):\n",
    "                    distance = self._cluster_distance(clusters[i], clusters[j], X, dist_matrix)\n",
    "                    if distance < min_distance:\n",
    "                        min_distance = distance\n",
    "                        merge_i, merge_j = i, j\n",
    "            \n",
    "            # Combinar os clusters mais próximos\n",
    "            new_cluster = clusters[merge_i] + clusters[merge_j]\n",
    "            \n",
    "            # Salvar informações da fusão\n",
    "            self.merge_history.append((clusters[merge_i].copy(), clusters[merge_j].copy()))\n",
    "            self.distances.append(min_distance)\n",
    "            \n",
    "            step += 1\n",
    "            print(f\"Passo {step}: Combinar clusters {clusters[merge_i]} e {clusters[merge_j]}\")\n",
    "            print(f\"Distância: {min_distance:.3f}\")\n",
    "            \n",
    "            # Remover os clusters antigos e adicionar o novo\n",
    "            clusters = [clusters[k] for k in range(len(clusters)) if k != merge_i and k != merge_j]\n",
    "            clusters.append(new_cluster)\n",
    "            \n",
    "            print(f\"Clusters restantes: {clusters}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3699be72",
   "metadata": {},
   "source": [
    "## 5. Executando o Algoritmo no Dataset Simples\n",
    "\n",
    "Vamos aplicar nossa implementação nos dados simples para observar passo a passo como os clusters são formados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d20282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar a clusterização hierárquica com ligação simples\n",
    "hc_single = HierarchicalClustering(linkage='single')\n",
    "hc_single.fit(X_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc9365",
   "metadata": {},
   "source": [
    "## 6. Dendrogramas: Visualizando a Hierarquia\n",
    "\n",
    "Um **dendrograma** é a representação gráfica da hierarquia de clusters. É uma estrutura em forma de árvore que mostra a ordem e as distâncias nas quais os clusters foram combinados.\n",
    "\n",
    "### Interpretação do Dendrograma:\n",
    "- **Eixo horizontal**: Representa as observações ou clusters.\n",
    "- **Eixo vertical**: Representa a distância na qual os clusters foram unidos.\n",
    "- **Altura dos ramos**: Indica a dissimilaridade entre os clusters combinados.\n",
    "\n",
    "Vamos usar a implementação otimizada do SciPy para criar dendrogramas profissionais:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd1ce69",
   "metadata": {},
   "source": [
    "### Usando as Funções `linkage` e `fcluster` do SciPy\n",
    "\n",
    "O SciPy fornece funções otimizadas para clusterização hierárquica que são muito mais eficientes que nossa implementação educacional. As duas funções principais são:\n",
    "\n",
    "#### 1. Função `linkage(X, method)`\n",
    "\n",
    "A função `linkage` calcula a matriz de ligação que representa a hierarquia de clusters:\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "\n",
    "# Sintaxe básica\n",
    "linkage_matrix = linkage(X, method='ward')\n",
    "```\n",
    "\n",
    "**Parâmetros principais:**\n",
    "- `X`: matriz de dados (n_samples × n_features)\n",
    "- `method`: critério de ligação ('single', 'complete', 'average', 'ward')\n",
    "- `metric`: métrica de distância (padrão: 'euclidean')\n",
    "\n",
    "**Retorno:**\n",
    "- Matriz (n-1) × 4 onde cada linha representa uma fusão:\n",
    "  - Colunas 0 e 1: índices dos clusters sendo combinados\n",
    "  - Coluna 2: distância da fusão\n",
    "  - Coluna 3: número de observações no novo cluster\n",
    "\n",
    "#### 2. Função `fcluster(Z, t, criterion)`\n",
    "\n",
    "A função `fcluster` extrai clusters da matriz de ligação com base em um critério de corte:\n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "# Obter clusters com base na distância\n",
    "clusters = fcluster(linkage_matrix, t=3.0, criterion='distance')\n",
    "\n",
    "# Obter um número específico de clusters\n",
    "clusters = fcluster(linkage_matrix, t=3, criterion='maxclust')\n",
    "```\n",
    "\n",
    "**Parâmetros principais:**\n",
    "- `Z`: matriz de ligação (resultado de `linkage`)\n",
    "- `t`: valor do critério de corte\n",
    "- `criterion`: tipo de critério ('distance', 'maxclust', 'inconsistent')\n",
    "\n",
    "**Critérios de corte:**\n",
    "- `'distance'`: corta em uma altura específica do dendrograma\n",
    "- `'maxclust'`: força um número específico de clusters\n",
    "- `'inconsistent'`: baseado no coeficiente de inconsistência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95daf311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando scipy para criar dendrogramas profissionais\n",
    "# Diferentes critérios de ligação\n",
    "\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, method in enumerate(linkage_methods):\n",
    "    # Calcular a matriz de ligação\n",
    "    linkage_matrix = linkage(X_simple, method=method)\n",
    "    \n",
    "    # Criar o dendrograma\n",
    "    dendrogram(linkage_matrix, ax=axes[i], labels=[f'P{j}' for j in range(len(X_simple))])\n",
    "    axes[i].set_title(f'Dendrograma - {method.capitalize()} Linkage')\n",
    "    axes[i].set_xlabel('Pontos de Dados')\n",
    "    axes[i].set_ylabel('Distância')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681505ad",
   "metadata": {},
   "source": [
    "### Comparação dos Critérios de Ligação\n",
    "\n",
    "Cada critério de ligação produz diferentes estruturas de cluster:\n",
    "\n",
    "- **Single Linkage**: Tende a criar clusters elongados e pode sofrer do \"efeito corrente\" (chaining effect).\n",
    "- **Complete Linkage**: Produz clusters mais compactos e esféricos.\n",
    "- **Average Linkage**: Um meio-termo entre single e complete.\n",
    "- **Ward Linkage**: Minimiza a variância intra-cluster, similar ao objetivo do K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d6ce8",
   "metadata": {},
   "source": [
    "## 7. Determinando o Número de Clusters\n",
    "\n",
    "Uma das grandes vantagens da clusterização hierárquica é que podemos \"cortar\" o dendrograma em diferentes alturas para obter diferentes números de clusters. Isso é feito traçando uma linha horizontal através do dendrograma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4d6ce8_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar Ward linkage para o exemplo\n",
    "linkage_matrix = linkage(X_simple, method='ward')\n",
    "\n",
    "# Definir diferentes alturas de corte\n",
    "cut_heights = [2.0, 4.0, 6.0]\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "# Visualizar o dendrograma com diferentes cortes\n",
    "plt.figure(figsize=(10, 6))\n",
    "dendrogram(linkage_matrix, labels=[f'P{j}' for j in range(len(X_simple))])\n",
    "\n",
    "for height, color in zip(cut_heights, colors):\n",
    "    plt.axhline(y=height, color=color, linestyle='--', label=f'Corte em {height}')\n",
    "\n",
    "plt.title('Dendrograma com Linhas de Corte')\n",
    "plt.xlabel('Pontos de Dados')\n",
    "plt.ylabel('Distância')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mxozms9yqro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predição de clusters baseada em diferentes linhas de corte\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "print(\"Predição de clusters para diferentes alturas de corte:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for i, height in enumerate(cut_heights):\n",
    "    # Obter clusters para a altura de corte específica\n",
    "    clusters = fcluster(linkage_matrix, height, criterion='distance')\n",
    "    n_clusters = len(np.unique(clusters))\n",
    "    \n",
    "    print()\n",
    "    print(f\"Altura de corte: {height}\")\n",
    "    print(f\"Número de clusters: {n_clusters}\")\n",
    "    \n",
    "    # Mostrar quais pontos pertencem a cada cluster\n",
    "    for cluster_id in np.unique(clusters):\n",
    "        points = [f\"P{j}\" for j in range(len(X_simple)) if clusters[j] == cluster_id]\n",
    "        print(f\"  Cluster {cluster_id}: {points}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ddf1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização dos clusters resultantes\n",
    "fig, axes = plt.subplots(1, len(cut_heights), figsize=(15, 4))\n",
    "\n",
    "for i, height in enumerate(cut_heights):\n",
    "    clusters = fcluster(linkage_matrix, height, criterion='distance')\n",
    "    scatter = axes[i].scatter(X_simple[:, 0], X_simple[:, 1], c=clusters, s=100, alpha=0.7, cmap='viridis')\n",
    "    \n",
    "    # Adicionar rótulos dos pontos\n",
    "    for j, (x, y) in enumerate(X_simple):\n",
    "        axes[i].annotate(f'P{j}', (x, y), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    axes[i].set_title(f'Clusters (corte = {height})\\n{len(np.unique(clusters))} clusters')\n",
    "    axes[i].set_xlabel('Feature 1')\n",
    "    axes[i].set_ylabel('Feature 2')\n",
    "    axes[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee707fe",
   "metadata": {},
   "source": [
    "## 8. Aplicação ao Dataset Iris\n",
    "\n",
    "Agora vamos aplicar a clusterização hierárquica ao dataset Iris e comparar os resultados com o K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b9b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset Iris\n",
    "iris = load_iris()\n",
    "X_iris = iris.data[:, 2:]  # Usar comprimento e largura da pétala\n",
    "y_true = iris.target\n",
    "\n",
    "# Aplicar diferentes métodos de ligação\n",
    "methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for i, method in enumerate(methods):\n",
    "    # Calcular a matriz de ligação\n",
    "    linkage_matrix = linkage(X_iris, method=method)\n",
    "    \n",
    "    # Dendrograma\n",
    "    dendrogram(linkage_matrix, ax=axes[0, i], no_labels=True)\n",
    "    axes[0, i].set_title(f'Dendrograma - {method.capitalize()}')\n",
    "    \n",
    "    # Obter 3 clusters\n",
    "    clusters = fcluster(linkage_matrix, 3, criterion='maxclust')\n",
    "    \n",
    "    # Plotar os clusters\n",
    "    scatter = axes[1, i].scatter(X_iris[:, 0], X_iris[:, 1], c=clusters, s=50, alpha=0.7, cmap='viridis')\n",
    "    axes[1, i].set_title(f'Clusters - {method.capitalize()}')\n",
    "    axes[1, i].set_xlabel('Comprimento da Pétala')\n",
    "    axes[1, i].set_ylabel('Largura da Pétala')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5cdf49",
   "metadata": {},
   "source": [
    "### Avaliação dos Resultados\n",
    "\n",
    "Vamos calcular a taxa de acertos para cada método de ligação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d064c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def calculate_purity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula a pureza dos clusters comparando com os rótulos verdadeiros.\n",
    "    \"\"\"\n",
    "    correct_predictions = 0\n",
    "    n_samples = len(y_true)\n",
    "    \n",
    "    for cluster_id in np.unique(y_pred):\n",
    "        mask = (y_pred == cluster_id)\n",
    "        if np.sum(mask) > 0:\n",
    "            dominant_label = mode(y_true[mask], keepdims=True)[0][0]\n",
    "            correct_predictions += np.sum(y_true[mask] == dominant_label)\n",
    "    \n",
    "    return correct_predictions / n_samples\n",
    "\n",
    "print(\"Comparação dos métodos de ligação no dataset Iris:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for method in methods:\n",
    "    linkage_matrix = linkage(X_iris, method=method)\n",
    "    clusters = fcluster(linkage_matrix, 3, criterion='maxclust')\n",
    "    purity = calculate_purity(y_true, clusters)\n",
    "    print(f\"{method.capitalize():12} Linkage: {purity:.1%} de acertos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf0c30",
   "metadata": {},
   "source": [
    "### Exercício 1: Implementação do Average Linkage\n",
    "\n",
    "Complete a implementação da nossa classe `HierarchicalClustering` adicionando o método **Average Linkage**. Em seguida, teste todos os três métodos de ligação (single, complete, average) no dataset simples (`X_simple`) e compare os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4753c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55661888",
   "metadata": {},
   "source": [
    "### Exercício 2: Análise do Dataset Wine - Seleção de Features e Comparação de Métodos\n",
    "\n",
    "Aplique a clusterização hierárquica do SciPy ao dataset Wine. Primeiro, você deve selecionar um bom par de features para visualização bidimensional, depois comparar diferentes métodos de ligação.\n",
    "\n",
    "**Tarefas:**\n",
    "1. Carregue o dataset Wine e explore suas features\n",
    "2. Selecione as duas melhores features para visualização (analise correlações, variâncias, etc.)\n",
    "3. Aplique os 4 métodos de ligação ('single', 'complete', 'average', 'ward') usando `scipy.cluster.hierarchy.linkage`\n",
    "4. Crie dendrogramas para cada método\n",
    "5. Determine visualmente qual método produz a melhor separação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9257b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = load_wine()\n",
    "X_wine = wine.data\n",
    "y_wine = wine.target\n",
    "\n",
    "print(\"Dataset Wine:\")\n",
    "print(f\"Shape: {X_wine.shape}\")\n",
    "print(f\"Features: {wine.feature_names}\")\n",
    "print(f\"Classes: {wine.target_names}\")\n",
    "\n",
    "# 2. Análise das features para seleção\n",
    "# Seu código aqui\n",
    "\n",
    "# 3. Seleção das duas melhores features\n",
    "# Seu código aqui\n",
    "\n",
    "# 4. Aplicação dos métodos de ligação e criação dos dendrogramas\n",
    "# Seu código aqui\n",
    "\n",
    "# 5. Análise visual e determinação do melhor método\n",
    "# Seu código aqui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ii7gm22sbpi",
   "metadata": {},
   "source": [
    "### Exercício 3: Determinação do Número Ótimo de Clusters\n",
    "\n",
    "Com base no melhor método de ligação identificado no Exercício 2, determine o número ótimo de clusters para o dataset Wine usando análise visual do dendrograma e validação com os rótulos verdadeiros.\n",
    "\n",
    "**Tarefas:**\n",
    "1. Use o melhor método identificado no exercício anterior\n",
    "2. Crie um dendrograma detalhado com linha de corte ajustável\n",
    "3. Teste diferentes números de clusters (2, 3, 4, 5) usando `fcluster`\n",
    "4. Para cada número de clusters, visualize os clusters no scatter plot\n",
    "5. Determine o número ótimo de clusters justificando sua escolha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bkdb8je80qr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercício 3: Determinação do Número Ótimo de Clusters\n",
    "\n",
    "# 1. Use o melhor método do exercício anterior\n",
    "# best_method = '...'\n",
    "\n",
    "# 2. Crie dendrograma com diferentes linhas de corte\n",
    "# Seu código aqui\n",
    "\n",
    "# 3. Teste diferentes números de clusters\n",
    "n_clusters_to_test = [2, 3, 4, 5]\n",
    "\n",
    "print(\"Análise do número ótimo de clusters:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Seu código aqui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
